{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bb873e-562a-4d60-9536-b6b88aecb5c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Working with video on GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6c8277d-29b3-4172-8600-fcbbbbdbafd2",
   "metadata": {},
   "source": [
    "**Notebook 3 of 3**<br><br>\n",
    "This tutorial provides a high-level overview of working with Intel GPUs in OpenVINO. It shows how to use Query Device to list system GPUs and check their properties, and it explains some of the key properties. It shows how to compile a model on GPU with performance hints. \n",
    "\n",
    "The tutorial also shows example commands for benchmark_app that can be run to compare GPU performance in different configurations. It also provides the code for a basic end-to-end application that compiles a model on GPU and uses it to run inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e923f-e579-4261-b554-2ca0acf32df5",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "Average time to complete 25min\n",
    "\n",
    "By the end of this tutorial you should be able to:\n",
    "* Get information about the GPUs we want to run on.\n",
    "* Highlight the difference between running the inference on GPU vs CPU.\n",
    "* Convert a model from Tensorflow and compile it on the GPU\n",
    "* Perform a simple video inferencing application on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94befae9-1ea5-446e-913b-5505640a33ce",
   "metadata": {},
   "source": [
    "## What you will need for this tutorial\n",
    "\n",
    "* See the [introduction document](https://uottawa-it-research-teaching.github.io/machinelearning/) for general requirements and how Jupyter notebooks work.\n",
    "* We'll need Pandas for convenient data handling. It's a very powerful Python package that can read CSV and Excel files. It also has very good data manipulation capabilities which come in use for data cleaning.\n",
    "* openVINO 2023 or later\n",
    "* We will use scikit learn as our machine learning package. Scikit-learn provides simple and efficient tools for data mining and analysis.\n",
    "* numpy. Numpy provides support for large, multi-dimentional arrays and matrices.\n",
    "* seaborn. Provides an intuitive and attractive interface for creating informative and visually appealing statistical graphics.\n",
    "* matplotlib. Allows the generation of plots and charts.\n",
    "* requests. Handles Http requests and responses when downloading datasets or pre-trained models.\n",
    "* ipywidgets. Allows the creation of interactive plota, graphs and other visualizations, as well as control the execution of code.\n",
    "* The data files that should have come with this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c166d-ab84-4328-a6b3-08fbe1191c21",
   "metadata": {},
   "source": [
    "## RDM best practices\n",
    "\n",
    "Good data handling for machine learning begins with good Research Data Managment (RDM). The quality of your source data will impact the outcome of your results, just like the reproducibility of your results will depend on the quality of your data sources, in addition to how you organize the data so that other people (and machines!) can understand and reuse it.\n",
    "\n",
    "We also need to respect a few research data management best practices along the way, these best practices are recommended by the Digital Research Alliance of Canada. In the first tutorial we encouraged you to resepct two RDM best practices:\n",
    "\n",
    "* SAVE YOUR RAW DATA IN ORIGINAL FORMAT<br>\n",
    "* BACKUP YOUR DATA (3-2-1 rule)<br>\n",
    "\n",
    "These practices should apply in this tutorial as well, but we will also look at best practices of data description, documentation and file naming that will streamline your data processing and project management. \n",
    "\n",
    "DESCRIBE YOUR DATA\n",
    "\n",
    "* Machine Friendly: Describe your dataset with a metadata standard for discovery.\n",
    "* Human Friendly: Describe your variables, so your colleagues will understand what you meant. Data without good metadata is useless. Give your variables clear names.\n",
    "* Do not leave cells blank -use numeric values clearly out of range to define missing (e.g. '99999') or not applicable (e.g. '88888') data anddescribe these in your data dictionary.\n",
    "* Convert your data to open, non-proprietary formats \n",
    "* Name your files well with basic meta-data in the file names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7253ed-d8a1-4475-8e83-263336639157",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab1f9f13-63a4-4b2d-95e3-9a82c1e9cebd",
   "metadata": {},
   "source": [
    "Originally, graphic processing units (GPUs) began as specialized chips, developed to accelerate the rendering of computer graphics. In contrast to CPUs, which have few but powerful cores, GPUs have many more specialized cores, making them ideal for workloads that can be parallelized into simpler tasks. Nowadays, one such workload is deep learning, where GPUs can easily accelerate inference of neural networks by splitting operations across multiple cores.\n",
    "\n",
    "In this tutorial, we will use the **MobileNet V2** model due to it's light weight and ease of use.  This model and others like it tend to have relatively simple architectures and fewer parameters compared to larger, more complex models.  Because of it's light weight and simplicity, it's is actually best suited for CPU devices.  Complex Deep Learning models requiring more parallel will run more efficiently on GPU devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f35f17-5209-43d6-b3b5-db35efb1f42e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Checking GPUs with Query Device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c84c37c2-b24b-4c9f-9ab8-d3a60b8393ec",
   "metadata": {},
   "source": [
    "In this section, we will see how to list the available GPUs and check their properties. Some of the key properties will also be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e91abc-6118-4532-9817-6ef44c51b0c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### List GPUs with core.available_devices\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e76b6152-3915-415a-987b-c30935462007",
   "metadata": {},
   "source": [
    "OpenVINO Runtime provides the `available_devices` method for checking which devices are available for inference. The following code will output a list of compatible OpenVINO devices, in which Intel GPUs should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8260bef-63c4-45f3-9ffd-4cc3ac892680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPU', 'GPU.0', 'GPU.1', 'GPU.2']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "core.available_devices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a95aef31-bf6f-4d7e-85a5-97dc6bead645",
   "metadata": {},
   "source": [
    "Note that GPU devices are numbered starting at 0, where the integrated GPU always takes the id `0` if the system has one. For instance, if the system has a CPU, an integrated and discrete GPU, we should expect to see a list like this: `['CPU', 'GPU.0', 'GPU.1']`. To simplify its use, the \"GPU.0\" can also be addressed with just \"GPU\". For more details, see the [Device Naming Convention](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_GPU.html#device-naming-convention) section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e266c79-7bb9-4907-9204-87b688087cbf",
   "metadata": {},
   "source": [
    "If the GPUs are installed correctly on the system and still do not appear in the list, follow the steps described [here](https://docs.openvino.ai/latest/openvino_docs_install_guides_configurations_for_intel_gpu.html) to configure your GPU drivers to work with OpenVINO. Once we have the GPUs working with OpenVINO, we can proceed with the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0491c07-0b6d-483f-a4d6-d3f7b5ec0c81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check Properties with core.get_property"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54399a50-31cf-48bd-bf98-6dea0c0b1b20",
   "metadata": {},
   "source": [
    "To get information about the GPUs, we can use device properties. In OpenVINO, devices have properties that describe their characteristics and configuration. Each property has a name and associated value that can be queried with the `get_property` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf7509b6-67e3-46fb-bb23-5f46c58b0fc1",
   "metadata": {},
   "source": [
    "To get the value of a property, such as the device name, we can use the `get_property` method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eacdd5be-5d75-41d5-a51b-a376cb063b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gfx90a:sramecc+:xnack- (dGPU)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"GPU.1\"\n",
    "core.get_property(device, \"FULL_DEVICE_NAME\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aac3129a-129f-49aa-aba0-71ae1e892ada",
   "metadata": {},
   "source": [
    "Each device also has a specific property called `SUPPORTED_PROPERTIES`, that enables viewing all the available properties in the device. We can check the value for each property by simply looping through the dictionary returned by `core.get_property(\"GPU\", \"SUPPORTED_PROPERTIES\")` and then querying for that property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb073e8-c997-40db-92fa-93334731285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU.1 SUPPORTED_PROPERTIES:\n",
      "\n",
      "AVAILABLE_DEVICES               : ['0', '1', '2']\n",
      "RANGE_FOR_ASYNC_INFER_REQUESTS  : (1, 2, 1)\n",
      "RANGE_FOR_STREAMS               : (1, 2)\n",
      "OPTIMAL_BATCH_SIZE              : 1\n",
      "MAX_BATCH_SIZE                  : 1\n",
      "DEVICE_ARCHITECTURE             : GPU: vendor=0x1002 arch=gfx90a:sramecc+:xnack-\n",
      "FULL_DEVICE_NAME                : gfx90a:sramecc+:xnack- (dGPU)\n",
      "DEVICE_UUID                     : 00000000000000000000000000000000\n",
      "DEVICE_LUID                     : 0000000000000000\n",
      "DEVICE_TYPE                     : Type.DISCRETE\n",
      "DEVICE_GOPS                     : {<Type: 'float16'>: 0.0, <Type: 'float32'>: 0.0, <Type: 'int8_t'>: 0.0, <Type: 'uint8_t'>: 0.0}\n",
      "OPTIMIZATION_CAPABILITIES       : ['FP32', 'BIN', 'FP16', 'INT8', 'EXPORT_IMPORT']\n",
      "GPU_DEVICE_TOTAL_MEM_SIZE       : 68702699520\n",
      "GPU_UARCH_VERSION               : unknown\n",
      "GPU_EXECUTION_UNITS_COUNT       : 104\n",
      "GPU_MEMORY_STATISTICS           : {}\n",
      "PERF_COUNT                      : False\n",
      "MODEL_PRIORITY                  : Priority.MEDIUM\n",
      "GPU_HOST_TASK_PRIORITY          : Priority.MEDIUM\n",
      "GPU_QUEUE_PRIORITY              : Priority.MEDIUM\n",
      "GPU_QUEUE_THROTTLE              : Priority.MEDIUM\n",
      "GPU_ENABLE_LOOP_UNROLLING       : True\n",
      "GPU_DISABLE_WINOGRAD_CONVOLUTION: False\n",
      "CACHE_DIR                       : \n",
      "CACHE_MODE                      : UNSUPPORTED TYPE\n",
      "PERFORMANCE_HINT                : PerformanceMode.LATENCY\n",
      "EXECUTION_MODE_HINT             : ExecutionMode.PERFORMANCE\n",
      "COMPILATION_NUM_THREADS         : 128\n",
      "NUM_STREAMS                     : 1\n",
      "PERFORMANCE_HINT_NUM_REQUESTS   : 0\n",
      "INFERENCE_PRECISION_HINT        : <Type: 'float16'>\n",
      "ENABLE_CPU_PINNING              : False\n",
      "DEVICE_ID                       : 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"{device} SUPPORTED_PROPERTIES:\\n\")\n",
    "supported_properties = core.get_property(device, \"SUPPORTED_PROPERTIES\")\n",
    "indent = len(max(supported_properties, key=len))\n",
    "\n",
    "for property_key in supported_properties:\n",
    "    if property_key not in ('SUPPORTED_METRICS', 'SUPPORTED_CONFIG_KEYS', 'SUPPORTED_PROPERTIES'):\n",
    "        try:\n",
    "            property_val = core.get_property(device, property_key)\n",
    "        except TypeError:\n",
    "            property_val = 'UNSUPPORTED TYPE'\n",
    "        print(f\"{property_key:<{indent}}: {property_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88546b71-2ae8-4519-b9b7-a54f444e204f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Brief Descriptions of Key Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524096f-cc77-4dca-9e52-e5a3c2c0523b",
   "metadata": {},
   "source": [
    "Each device has several properties as seen in the last command. Some of the key properties are:\n",
    "\n",
    "* `FULL_DEVICE_NAME` -Â The product name of the GPU and whether it is an integrated or discrete GPU (iGPU or dGPU).\n",
    "* `OPTIMIZATION_CAPABILITIES` - The model data types (INT8, FP16, FP32, etc) that are supported by this GPU.\n",
    "* `GPU_EXECUTION_UNITS_COUNT` - The execution cores available in the GPU's architecture, which is a relative measure of the GPU's processing power.\n",
    "* `RANGE_FOR_STREAMS` - The number of processing streams available on the GPU that can be used to execute parallel inference requests. When compiling a model in LATENCY or THROUGHPUT mode, OpenVINO will automatically select the best number of streams for low latency or high throughput.\n",
    "* `PERFORMANCE_HINT` - A high-level way to tune the device for a specific performance metric, such as latency or throughput, without worrying about device-specific settings.\n",
    "* `CACHE_DIR` - The directory where the model cache data is stored to speed up compilation time.\n",
    "\n",
    "\n",
    "To learn more about devices and properties, see the [Query Device Properties](https://docs.openvino.ai/latest/openvino_docs_OV_UG_query_api.html) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131bd0f9-9b1e-4fc1-ad09-3960e1d6c50f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compiling a Model on GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cea15935-2373-4029-8bc0-998558a6defe",
   "metadata": {},
   "source": [
    "Now, we know how to list the GPUs in the system and check their properties. We can easily use one for compiling and running models with OpenVINO [GPU plugin](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_GPU.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4600db-faea-49cc-b9b9-93415223a38f",
   "metadata": {},
   "source": [
    "### Download and Convert a Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "821e29d1-6021-4552-a926-cb35bd6af776",
   "metadata": {},
   "source": [
    "This tutorial uses the `ssdlite_mobilenet_v2` model. The `ssdlite_mobilenet_v2` model is used for object detection. The model was trained on [Common Objects in Context (COCO)](https://cocodataset.org/#home) dataset version with 91 categories of object. For details, see the [paper](https://arxiv.org/abs/1801.04381)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad24a39d-8819-47a3-9c91-0f80e92649b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Download and unpack the Model\n",
    "\n",
    "Use the `download_file` function from the `notebook_utils` to download an archive with the model. It automatically creates a directory structure and downloads the selected model. This step is skipped if the package is already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061a741f-2fc7-430e-b2f7-21d229533447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "import notebook_utils as utils\n",
    "\n",
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = Path(\"./model\").expanduser()\n",
    "\n",
    "model_name = \"ssdlite_mobilenet_v2\"\n",
    "archive_name = Path(f\"{model_name}_coco_2018_05_09.tar.gz\")\n",
    "\n",
    "# Download the archive\n",
    "downloaded_model_path = base_model_dir / archive_name\n",
    "if not downloaded_model_path.exists():\n",
    "    model_url = f\"http://download.tensorflow.org/models/object_detection/{archive_name}\"\n",
    "    utils.download_file(model_url, downloaded_model_path.name, downloaded_model_path.parent)\n",
    "\n",
    "# Unpack the model\n",
    "tf_model_path = base_model_dir / archive_name.with_suffix(\"\").stem / \"frozen_inference_graph.pb\"\n",
    "if not tf_model_path.exists():\n",
    "    with tarfile.open(downloaded_model_path) as file:\n",
    "        file.extractall(base_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaa8d2-724b-47f5-991d-22bbe6d1c994",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Convert the Model to OpenVINO IR format\n",
    "\n",
    "Use Model Optimizer to convert the model to OpenVINO IR with `FP16` precision. The models are saved to the `model/ir_model/` directory. For more information about Model Optimizer, see the [Model Optimizer Developer Guide](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd75b02b-d0a1-422f-b3db-42558ccf5749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read IR model from model/ir_model/ssdlite_mobilenet_v2_fp16.xml\n"
     ]
    }
   ],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "from openvino.tools.mo.front import tf as ov_tf_front\n",
    "\n",
    "precision = 'FP16'\n",
    "\n",
    "# The output path for the conversion.\n",
    "model_path = base_model_dir / 'ir_model' / f'{model_name}_{precision.lower()}.xml'\n",
    "\n",
    "trans_config_path = Path(ov_tf_front.__file__).parent / \"ssd_v2_support.json\"\n",
    "pipeline_config = base_model_dir / archive_name.with_suffix(\"\").stem / \"pipeline.config\"\n",
    "\n",
    "model = None\n",
    "if not model_path.exists():\n",
    "    model = mo.convert_model(input_model=tf_model_path,\n",
    "                             output_dir=base_model_dir / 'ir_model',\n",
    "                             model_name=f'{model_name}_{precision.lower()}',\n",
    "                             input_shape=[1, 300, 300, 3],\n",
    "                             layout='NHWC',\n",
    "                             compress_to_fp16=True if precision == 'FP16' else False,\n",
    "                             transformations_config=trans_config_path,\n",
    "                             tensorflow_object_detection_api_pipeline_config=pipeline_config,\n",
    "                             reverse_input_channels=True)\n",
    "    serialize(model, str(model_path))\n",
    "    print(\"IR model saved to {}\".format(model_path))\n",
    "else:\n",
    "    print(\"Read IR model from {}\".format(model_path))\n",
    "    model = core.read_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc043aa-8729-45bc-9e98-6b3daea9f271",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile with Default Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6999d5e8-3429-4e2b-a845-c5569ec1895f",
   "metadata": {},
   "source": [
    "When the model is ready, first we need to read it, using the `read_model` method. Then, we can use the `compile_model` method and specify the name of the device we want to compile the model on, in this case, \"GPU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f7d00e-ebe5-456d-8643-112e9d8c860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01fcf1-ce12-4ca0-a990-206c415c132f",
   "metadata": {},
   "source": [
    "If you have multiple GPUs in the system, you can specify which one to use by using \"GPU.0\", \"GPU.1\", etc. Any of the device names returned by the `available_devices` method are valid device specifiers. You may also use \"AUTO\", which will automatically select the best device for inference (which is often the GPU)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22069d88-1d29-4978-ba32-2613f9d2143d",
   "metadata": {},
   "source": [
    "Depending on the model used, device-specific optimizations and network compilations can cause the compile step to be time-consuming, especially with larger models, which may lead to bad user experience in the application, in which they are used. To solve this, OpenVINO can cache the model once it is compiled on supported devices and reuse it in later `compile_model` calls by simply setting a cache folder beforehand. For instance, to cache the same model we compiled above, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ee3bba-94e6-4a82-9c79-31d86a89ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache enabled (first time) - compile time: 6.006357908248901s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Create cache folder\n",
    "cache_folder = Path(\"cache\")\n",
    "cache_folder.mkdir(exist_ok=True)\n",
    "\n",
    "start = time.time()\n",
    "core = Core()\n",
    "\n",
    "# Set cache folder\n",
    "core.set_property({'CACHE_DIR': cache_folder})\n",
    "#core.set_property({'CACHE_MODE': ''})\n",
    "\n",
    "# Compile the model as before\n",
    "model = core.read_model(model=model_path)\n",
    "compiled_model = core.compile_model(model, device)\n",
    "# compiled_model = core.compile_model(model, \"CPU\")\n",
    "print(f\"Cache enabled (first time) - compile time: {time.time() - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd1496-996d-49d8-80b9-3bfb3dda9729",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Application Using GPUs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53a25247-a69f-40a6-be52-ea6de89bf470",
   "metadata": {},
   "source": [
    "We will now show an end-to-end object detection example using GPUs in OpenVINO. The application compiles a model on GPU with the \"THROUGHPUT\" hint, then loads a video and preprocesses every frame to convert them to the shape expected by the model. Once the frames are loaded, it sets up an asynchronous pipeline, performs inference and saves the detections found in each frame. The detections are then drawn on their corresponding frame and saved as a video, which is displayed at the end of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1686090-2a2e-4ae6-8f22-7ef458ce221b",
   "metadata": {},
   "source": [
    "### Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55e0b948-ec1f-4ab8-9418-103dc8f06c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "from openvino.runtime import AsyncInferQueue, Core, InferRequest\n",
    "\n",
    "# Instantiate OpenVINO Runtime\n",
    "core = Core()\n",
    "#core.available_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadfb30e-d6d3-4ed3-8e7e-f1a843ce2ec0",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8648ae4f-2c73-4525-ace0-1c71afb18ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input shape: 1 300 300 3\n"
     ]
    }
   ],
   "source": [
    "# Read model and compile it on GPU in THROUGHPUT mode\n",
    "model = core.read_model(model=model_path)\n",
    "device_name = \"GPU.2\"\n",
    "\n",
    "# compiled_model = core.compile_model(model=model, device_name=device_name, config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "compiled_model = core.compile_model(model=model, device_name=device_name)\n",
    "\n",
    "# Get the input and output nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# Get the input size\n",
    "num, height, width, channels = input_layer.shape\n",
    "print('Model input shape:', num, height, width, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083bce3d-647d-46d1-8540-4a2935bc829f",
   "metadata": {},
   "source": [
    "### Load and Preprocess Video Frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcee85d9-eab1-4a94-bb2f-d5217fd7cdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading video...\n",
      "Video loaded!\n",
      "Frame shape:  (1, 300, 300, 3)\n",
      "Number of frames:  288\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../data/video/Coco Walking in Berkeley.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load video\n",
    "video_file = \"../data/video/Coco Walking in Berkeley.mp4\"\n",
    "video = cv2.VideoCapture(video_file)\n",
    "framebuf = []\n",
    "\n",
    "# Go through every frame of video and resize it\n",
    "print('Loading video...')\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        print('Video loaded!')\n",
    "        video.release()\n",
    "        break\n",
    "    \n",
    "    # Preprocess frames - convert them to shape expected by model\n",
    "    input_frame = cv2.resize(src=frame, dsize=(width, height), interpolation=cv2.INTER_AREA)\n",
    "    input_frame = np.expand_dims(input_frame, axis=0)\n",
    "\n",
    "    # Append frame to framebuffer\n",
    "    framebuf.append(input_frame)\n",
    "    \n",
    "\n",
    "print('Frame shape: ', framebuf[0].shape)\n",
    "print('Number of frames: ', len(framebuf))\n",
    "\n",
    "# Show original video file\n",
    "# If the video does not display correctly inside the notebook, please open it with your favorite media player\n",
    "Video(video_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003286a-8349-46f6-9d38-c6bb959e892a",
   "metadata": {},
   "source": [
    "### Define Model Output Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "621b919e-c120-42f9-89eb-ffd7b810c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model's labelmap (this model uses COCO classes)\n",
    "classes = [\n",
    "    \"background\", \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\",\n",
    "    \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"street sign\", \"stop sign\",\n",
    "    \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\",\n",
    "    \"bear\", \"zebra\", \"giraffe\", \"hat\", \"backpack\", \"umbrella\", \"shoe\", \"eye glasses\",\n",
    "    \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n",
    "    \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\",\n",
    "    \"plate\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
    "    \"couch\", \"potted plant\", \"bed\", \"mirror\", \"dining table\", \"window\", \"desk\", \"toilet\",\n",
    "    \"door\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\",\n",
    "    \"toaster\", \"sink\", \"refrigerator\", \"blender\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "    \"teddy bear\", \"hair drier\", \"toothbrush\", \"hair brush\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad10b97-e32e-4885-af1d-0636e970b707",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6ed58-8b3f-415e-a733-a7517e200b12",
   "metadata": {},
   "source": [
    "#### Callback Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1268e1a4-b530-4b39-b79d-d6d5f988a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback function that runs every time the asynchronous pipeline completes inference on a frame\n",
    "def completion_callback(infer_request: InferRequest, frame_id: int) -> None:\n",
    "    global frame_number\n",
    "    stop_time = time.time()\n",
    "    frame_number += 1\n",
    "\n",
    "    predictions = next(iter(infer_request.results.values()))\n",
    "    results[frame_id] = predictions[:10]  # Grab first 10 predictions for this frame\n",
    "    \n",
    "    total_time = stop_time - start_time\n",
    "    frame_fps[frame_id] = frame_number / total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f6aba-b3ba-478b-b02f-6b438b575c33",
   "metadata": {},
   "source": [
    "#### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e8eae2-20a9-43dc-8774-444ad0179136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create asynchronous inference queue with optimal number of infer requests\n",
    "infer_queue = AsyncInferQueue(compiled_model)\n",
    "infer_queue.set_callback(completion_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47ae59-b86d-46c4-b219-e7beccd5f796",
   "metadata": {},
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59f493dd-4cb7-4ae2-b117-7ba41a950d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With model caching:\n",
      "Total time to infer all frames: 1.207s\n",
      "Time per frame: 0.004190s (238.65 FPS)\n"
     ]
    }
   ],
   "source": [
    "# Perform inference on every frame in the frame buffer\n",
    "results = {}\n",
    "frame_fps = {}\n",
    "frame_number = 0\n",
    "start_time = time.time()\n",
    "for i, input_frame in enumerate(framebuf):\n",
    "    infer_queue.start_async({0: input_frame}, i)\n",
    "\n",
    "infer_queue.wait_all()  # Wait until all inference requests in the AsyncInferQueue are completed\n",
    "stop_time = time.time()\n",
    "\n",
    "# Calculate total inference time and FPS\n",
    "total_time = stop_time - start_time\n",
    "fps = len(framebuf) / total_time\n",
    "time_per_frame = 1 / fps \n",
    "print(\"With model caching:\")\n",
    "print(f'Total time to infer all frames: {total_time:.3f}s')\n",
    "print(f'Time per frame: {time_per_frame:.6f}s ({fps:.2f} FPS)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd90b68-ae14-4fe9-98c7-e8d23146c82c",
   "metadata": {},
   "source": [
    "### Process Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ccba843-e40b-43a8-b2b4-8ebd67bc1424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video loaded!\n",
      "object detected: dog\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"../data/video/Coco Walking in Berkeley.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set minimum detection threshold\n",
    "min_thresh = .6\n",
    "\n",
    "# Load video\n",
    "video = cv2.VideoCapture(video_file)\n",
    "\n",
    "# Get video parameters\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = int(video.get(cv2.CAP_PROP_FOURCC))\n",
    "Path('./output').mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Draw detection results on every frame of video and save as a new video file\n",
    "while video.isOpened():\n",
    "    current_frame = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        print('Video loaded!')\n",
    "        video.release()\n",
    "        break\n",
    "        \n",
    "    # Draw info at the top left such as current fps, the devices and the performance hint being used\n",
    "    cv2.putText(frame, f\"fps {str(round(frame_fps[current_frame], 2))}\", (5, 20), cv2.FONT_ITALIC, 0.6, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(frame, f\"device {device_name}\", (5, 40), cv2.FONT_ITALIC, 0.6, (0, 0, 0), 1, cv2.LINE_AA) \n",
    "    cv2.putText(frame, f\"hint {compiled_model.get_property('PERFORMANCE_HINT').name}\", (5, 60), cv2.FONT_ITALIC, 0.6, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    # prediction contains [image_id, label, conf, x_min, y_min, x_max, y_max] according to model\n",
    "    for prediction in np.squeeze(results[current_frame]):\n",
    "        if prediction[2] > min_thresh:\n",
    "            x_min = int(prediction[3] * frame_width)\n",
    "            y_min = int(prediction[4] * frame_height)\n",
    "            x_max = int(prediction[5] * frame_width)\n",
    "            y_max = int(prediction[6] * frame_height)\n",
    "            label = classes[int(prediction[1])]\n",
    "            \n",
    "            # Draw a bounding box with its label above it\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0,255,0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_ITALIC, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_ITALIC, 1, (255,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "print(f\"object detected:\", label)\n",
    "Video(\"../data/video/Coco Walking in Berkeley.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb6bb6-0d1c-4b0b-8a0b-efb5667816ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbbb4f28-2917-45ca-8fa0-8c670c0348bc",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how easy it is to use one or more GPUs in OpenVINO, check their properties, and even tailor the model performance through the different performance hints. It also provides a walk-through of a basic object detection application that uses a GPU and displays the detected bounding boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddbbcf8-2db5-4b2f-a07b-cef5b420a0df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
